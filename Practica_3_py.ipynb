{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Practica 3.py",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-HjdNDo1z7x"
      },
      "source": [
        "PRACTICA 3\n",
        "TAREA 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOdIMzRtRiyv",
        "outputId": "5b93d045-0176-41cc-f40c-94199d3d1ae1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('word2vec_sample')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim\n",
        "import numpy as np\n",
        "from nltk.data import find\n",
        "import statistics\n",
        "import itertools\n",
        "import scipy.spatial as s"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/word2vec_sample.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP67zjMhBode"
      },
      "source": [
        "stopwords = set(sw.words(\"english\"))\n",
        "def leskD(word, phrase):\n",
        "  senses = wn.synsets(word)\n",
        "  best_sense = senses[0]\n",
        "  max_ovelap = 0\n",
        "  word_tokens = set(word_tokenize(phrase))\n",
        "  context = []\n",
        "  for word in word_tokens:\n",
        "    if word not in stopwords:\n",
        "      context.append(word)\n",
        "\n",
        "  for sense in senses:\n",
        "    exam = word_tokenize(sense.definition())\n",
        "    for ex in sense.examples():\n",
        "      exam += word_tokenize(ex)\n",
        "    signature = []\n",
        "    for y in exam:\n",
        "      if y not in stopwords:\n",
        "        signature.append(y)\n",
        "    overlap = len(list(set(signature).intersection(context)))\n",
        "    if overlap > max_ovelap:\n",
        "      max_ovelap = overlap\n",
        "      best_sense = sense\n",
        "\n",
        "  print(best_sense.definition())\n",
        "  return best_sense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB1dOQf4DQvi",
        "outputId": "97719214-19d6-4f23-b13c-8cbbe4a3f2db"
      },
      "source": [
        "print(leskD(\"bank\", \"The van pulled up outside the bank and three masked men got out.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sloping land (especially the slope beside a body of water)\n",
            "Synset('bank.n.01')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1tWYlCzLr58"
      },
      "source": [
        "def dCos(x,y):\n",
        "    return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
        "\n",
        "def leskC(word, phrase):\n",
        "  word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
        "  model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)\n",
        "  v_mean =  model.vectors.mean(axis=0)\n",
        "  d=[]\n",
        "  dm=[]\n",
        "  context = []\n",
        "  senses = wn.synsets(word)\n",
        "  best_sense = senses[0]\n",
        "  max_ovelap = 0\n",
        "  word_tokens = set(word_tokenize(phrase))\n",
        "\n",
        "  for word in word_tokens:\n",
        "    if word not in stopwords:\n",
        "      if word in model.vocab:\n",
        "        context.append(model[word])\n",
        "      else:\n",
        "        context.append(v_mean)\n",
        "\n",
        "  v_cont = []\n",
        "  for column in range(len(context[0])):\n",
        "    aux = 0\n",
        "    for row in range(len(context)):\n",
        "      aux += context[row][column]\n",
        "    #v_cont.append(aux)\n",
        "    v_cont.append(aux/len(context[0]))\n",
        "  \n",
        "  for sense in senses:\n",
        "    exam = set(word_tokenize(sense.definition() + \" \"+ \" \".join(sense.examples()))).difference(stopwords)\n",
        "    signature=[]\n",
        "    for word in exam:\n",
        "      if word in model.vocab:\n",
        "        signature.append(model[word])\n",
        "      else:\n",
        "        signature.append(v_mean)\n",
        "    v_sig = []\n",
        "    for column in range(len(signature[0])):\n",
        "      aux = 0\n",
        "      for row in range(len(signature)):\n",
        "        aux += signature[row][column]\n",
        "      #v_sig.append(aux)\n",
        "      v_sig.append(aux/len(signature[0]))\n",
        "    d.append((sense, dCos(v_cont, v_sig)))\n",
        "  return sorted(d, key=lambda t:t[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBpiMLAZMCNA",
        "outputId": "c863458c-04f2-4dd8-9365-763213f41ff6"
      },
      "source": [
        "sentido = leskC(\"bass\", \"I went fishing for some sea bass.\")\n",
        "#sentido_t = sentido[0].definition()\n",
        "#print(sentido, sentido_t)\n",
        "print(sentido)\n",
        "print(sentido[0][0])\n",
        "print(sentido[0][0].definition())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(Synset('sea_bass.n.01'), 0.5321858971292431), (Synset('bass.n.08'), 0.5309086217649079), (Synset('freshwater_bass.n.01'), 0.4889060061784287), (Synset('bass.s.01'), 0.45519594654207735), (Synset('bass.n.02'), 0.346013269342772), (Synset('bass.n.06'), 0.3134628391845537), (Synset('bass.n.07'), 0.30911920173610075), (Synset('bass.n.03'), 0.28831348191634626), (Synset('bass.n.01'), 0.2786361774388861)]\n",
            "Synset('sea_bass.n.01')\n",
            "the lean flesh of a saltwater fish of the family Serranidae\n"
          ]
        }
      ]
    }
  ]
}